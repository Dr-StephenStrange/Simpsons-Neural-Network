{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import boto3\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tempfile\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In future versions of this I would not embed login credentials in the code, but for testing purposes it's ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "aws_access_key_id = \"ACCESS\"\n",
    "aws_secret_access_key = \"SECRET\"\n",
    "s3_bucket_name = \"ca1-simpsons\"\n",
    "dataset = \"simpsons_dataset\"\n",
    "\n",
    "percentage_to_use = 1 # used for testing code, 1 meaning 100%\n",
    "\n",
    "img_size = 64 # 64 pixels\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "input_shape = (img_size, img_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "map_characters = {0: 'abraham_grampa_simpson', 1: 'apu_nahasapeemapetilon', 2: 'bart_simpson',\n",
    "                  3: 'charles_montgomery_burns', 4: 'chief_wiggum', 5: 'comic_book_guy', 6: 'edna_krabappel',\n",
    "                  7: 'homer_simpson', 8: 'kent_brockman', 9: 'krusty_the_clown', 10: 'lisa_simpson',\n",
    "                  11: 'marge_simpson', 12: 'milhouse_van_houten', 13: 'moe_szyslak',\n",
    "                  14: 'ned_flanders', 15: 'nelson_muntz', 16: 'principal_skinner', 17: 'sideshow_bob'}\n",
    "\n",
    "num_classes = len(map_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Creating Spark Instance with setting the memory of worker and drivers to 28GB (t2.2xlarge EC2 instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell'\n",
    "\n",
    "conf = SparkConf().setAppName(\"TrainCNN\").set(\"spark.executor.memory\", \"28g\").set(\"spark.driver.memory\", \"28g\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Below function is quite complex, essentially it iterates through the S3 bucket and dataset for each character\n",
    "\n",
    "This time the s3 object connection is inside the function as each worker needs to have it's own connection in a distributed system.\n",
    "\n",
    "It then reads the image, resizes it to 64x64 pixels with 3 RGB channels\n",
    "\n",
    "And to normalize the values it divides by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def load_data(character_id, character_name, percentage=0.1):\n",
    "    data = []\n",
    "    labels = []\n",
    "    character_folder = f\"{s3_bucket_name}/simpsons_dataset/{character_name}\"\n",
    "    print(f\"Loading data from: {character_folder}\")\n",
    "\n",
    "    s3 = boto3.client(\"s3\", aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    all_objects = []\n",
    "    for result in paginator.paginate(Bucket=s3_bucket_name, Prefix=f\"simpsons_dataset/{character_name}\"):\n",
    "        all_objects += result.get(\"Contents\", [])\n",
    "\n",
    "    all_objects.sort(key=lambda x: x['LastModified'])\n",
    "\n",
    "    subset_objects = all_objects[:int(len(all_objects) * percentage)]\n",
    "\n",
    "    for obj in subset_objects:\n",
    "        img_path = obj['Key']\n",
    "        print(f\"Loading image: {img_path}\")\n",
    "\n",
    "        with tempfile.TemporaryFile() as fp:\n",
    "            s3.download_fileobj(s3_bucket_name, img_path, fp)\n",
    "            fp.seek(0)\n",
    "            img = cv2.imdecode(np.frombuffer(fp.read(), np.uint8), 1)\n",
    "\n",
    "        img = cv2.resize(img, (img_size, img_size)).astype('float32') / 255\n",
    "        data.append((img, character_id))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Distributed the list of characters to each worker in an RDD\n",
    "\n",
    "Then each item in the RDD from the load_data function is then flattened into a new RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "data_rdd = sc.parallelize(map_characters.items())\n",
    "data_rdd = data_rdd.flatMap(lambda x: load_data(x[0], x[1], percentage=percentage_to_use))\n",
    "print(\"Data Loading Time:\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now the master node collected the transformed data and trains the model on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "collected_data = data_rdd.collect() # Memory intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([item[0] for item in collected_data])\n",
    "Y = to_categorical([item[1] for item in collected_data], num_classes)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(map_characters), activation='softmax'))\n",
    "\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate=1e-3, decay_steps=1000, decay_rate=0.9)\n",
    "opt = Adam(learning_rate=lr_schedule)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "model.fit(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
    "          validation_data=(X_test, Y_test),\n",
    "          steps_per_epoch=len(X_train) / batch_size, epochs=epochs)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "\n",
    "print(f'Test accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")\n",
    "\n",
    "sc.stop()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
