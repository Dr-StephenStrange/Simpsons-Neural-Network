{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import boto3\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tempfile\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In future versions of this I would not embed login credentials in the code, but for testing purposes it's ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "aws_access_key_id = \"ACCESS\"\n",
    "aws_secret_access_key = \"SECRET\"\n",
    "s3_bucket_name = \"ca1-simpsons\"\n",
    "dataset = \"simpsons_dataset\"\n",
    "\n",
    "percentage_to_use = 0.1 # used for testing code, 1 meaning 100%\n",
    "\n",
    "img_size = 64 # 64 pixels\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "input_shape = (img_size, img_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "map_characters = {0: 'abraham_grampa_simpson', 1: 'apu_nahasapeemapetilon', 2: 'bart_simpson',\n",
    "                  3: 'charles_montgomery_burns', 4: 'chief_wiggum', 5: 'comic_book_guy', 6: 'edna_krabappel',\n",
    "                  7: 'homer_simpson', 8: 'kent_brockman', 9: 'krusty_the_clown', 10: 'lisa_simpson',\n",
    "                  11: 'marge_simpson', 12: 'milhouse_van_houten', 13: 'moe_szyslak',\n",
    "                  14: 'ned_flanders', 15: 'nelson_muntz', 16: 'principal_skinner', 17: 'sideshow_bob'}\n",
    "\n",
    "num_classes = len(map_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Connect to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\", aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Creating Spark Instance with setting the memory of worker and drivers to 28GB (t2.2xlarge EC2 instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell'\n",
    "\n",
    "conf = SparkConf().setAppName(\"TrainCNN\").set(\"spark.executor.memory\", \"28g\").set(\"spark.driver.memory\", \"28g\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Below function is quite complex, essentially it iterates through the S3 bucket and dataset for each character\n",
    "\n",
    "It then reads the image, resizes it to 64x64 pixels with 3 RGB channels\n",
    "\n",
    "And to normalize the values it divides by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def load_data(character_id, character_name, percentage=0.1):\n",
    "    data = []\n",
    "    character_folder = f\"{s3_bucket_name}/simpsons_dataset/{character_name}\"\n",
    "    print(f\"Loading data from: {character_folder}\")\n",
    "\n",
    "    s3 = boto3.client(\"s3\", aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    all_objects = []\n",
    "    for result in paginator.paginate(Bucket=s3_bucket_name, Prefix=f\"simpsons_dataset/{character_name}\"):\n",
    "        all_objects += result.get(\"Contents\", [])\n",
    "\n",
    "    all_objects.sort(key=lambda x: x['LastModified'])\n",
    "\n",
    "    # Select based on the defined percentage\n",
    "    subset_objects = all_objects[:int(len(all_objects) * percentage)]\n",
    "\n",
    "    for obj in subset_objects:\n",
    "        img_path = obj['Key']\n",
    "        print(f\"Loading image: {img_path}\")\n",
    "\n",
    "        with tempfile.TemporaryFile() as fp:\n",
    "            s3.download_fileobj(s3_bucket_name, img_path, fp)\n",
    "            fp.seek(0)\n",
    "            img = cv2.imdecode(np.frombuffer(fp.read(), np.uint8), 1)\n",
    "\n",
    "        img = cv2.resize(img, (img_size, img_size)).astype('float32') / 255\n",
    "        data.append((img, character_id))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Distributed the list of characters to each worker in an RDD\n",
    "\n",
    "Then each item in the RDD from the load_data function is then flattened into a new RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f'Map characters: {map_characters.items()}')\n",
    "\n",
    "data_rdd = sc.parallelize(map_characters.items())\n",
    "\n",
    "data_rdd = data_rdd.flatMap(lambda x: load_data(x[0], x[1], percentage=percentage_to_use))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"First few items in data_rdd:\")\n",
    "print(data_rdd.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_data_rdd, test_data_rdd = data_rdd.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Debugging: Print the number of items in train and test RDDs - Takes 20 minutes!!\n",
    "#print(f\"Number of items in train_data_rdd: {train_data_rdd.count()}\")\n",
    "#print(f\"Number of items in test_data_rdd: {test_data_rdd.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "apply a lambda function to each tuple in the train/test rdd to extract the labels and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_data = train_data_rdd.map(lambda x: x[0])\n",
    "train_labels = train_data_rdd.map(lambda x: x[1])\n",
    "test_data = test_data_rdd.map(lambda x: x[0])\n",
    "test_labels = test_data_rdd.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "training and testing character IDs into one-hot encoded format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "num_classes = len(map_characters)\n",
    "train_labels = to_categorical(train_labels.collect(), num_classes)\n",
    "test_labels = to_categorical(test_labels.collect(), num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Learning Schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "total_samples = train_data.count()\n",
    "iterations_per_epoch = int(round(total_samples / batch_size,0))\n",
    "decay_steps = int(iterations_per_epoch * epochs)\n",
    "initial_learning_rate = 1e-3\n",
    "\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Image Generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "CNN Model used in local mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(map_characters), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "local_model = Sequential()\n",
    "local_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "local_model.add(MaxPooling2D((2, 2)))\n",
    "local_model.add(BatchNormalization())\n",
    "local_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "local_model.add(MaxPooling2D((2, 2)))\n",
    "local_model.add(BatchNormalization())\n",
    "local_model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "local_model.add(MaxPooling2D((2, 2)))\n",
    "local_model.add(Flatten())\n",
    "local_model.add(Dense(128, activation='relu'))\n",
    "local_model.add(Dropout(0.5))\n",
    "local_model.add(Dense(len(map_characters), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This function is to allow each worker to train a model on a partition of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_cnn(partition):\n",
    "    train_data_partition = []\n",
    "    train_labels_partition = []\n",
    "\n",
    "    for data_label_tuple in partition:\n",
    "        train_data_partition.append(data_label_tuple[0])\n",
    "        train_labels_partition.append(to_categorical(data_label_tuple[1], num_classes))\n",
    "\n",
    "    train_data_partition = np.array(train_data_partition)\n",
    "    train_labels_partition = np.array(train_labels_partition)\n",
    "\n",
    "    partition_model = Sequential()\n",
    "    partition_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    partition_model.add(MaxPooling2D((2, 2)))\n",
    "    partition_model.add(BatchNormalization())\n",
    "    partition_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    partition_model.add(MaxPooling2D((2, 2)))\n",
    "    partition_model.add(BatchNormalization())\n",
    "    partition_model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    partition_model.add(MaxPooling2D((2, 2)))\n",
    "    partition_model.add(Flatten())\n",
    "    partition_model.add(Dense(128, activation='relu'))\n",
    "    partition_model.add(Dropout(0.5))\n",
    "    partition_model.add(Dense(len(map_characters), activation='softmax'))\n",
    "\n",
    "    # Compile the model with the Adam optimizer\n",
    "    partition_optimizer = Adam(learning_rate=lr_schedule)\n",
    "    partition_model.compile(optimizer=partition_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    partition_model.fit(datagen.flow(train_data_partition, train_labels_partition, batch_size=batch_size),\n",
    "                        steps_per_epoch=len(train_data_partition) / batch_size, epochs=epochs)\n",
    "\n",
    "    weights = partition_model.get_weights()\n",
    "    return [weights]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Training a model on partitions of data and Testing model with average combined weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "start_distributed_time = time.time()\n",
    "\n",
    "weights_rdd = data_rdd.mapPartitions(train_cnn)\n",
    "\n",
    "all_weights = weights_rdd.collect()\n",
    "\n",
    "average_weights = [np.zeros_like(w) for w in all_weights[0]]\n",
    "\n",
    "for weights in all_weights:\n",
    "    for i, w in enumerate(weights):\n",
    "        average_weights[i] += w\n",
    "\n",
    "num_workers = len(all_weights)\n",
    "\n",
    "for i in range(len(average_weights)):\n",
    "    average_weights[i] /= num_workers\n",
    "    \n",
    "model.set_weights(average_weights)\n",
    "\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "end_distributed_time = time.time()\n",
    "print(f\"Time taken for distributed training: {end_distributed_time - start_distributed_time:.2f} seconds\")\n",
    "\n",
    "test_data_array = np.array(test_data.collect())\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_data_array, test_labels)\n",
    "\n",
    "print(f'Test accuracy for distributed model: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "start_local_time = time.time()\n",
    "\n",
    "train_data_array = np.array(train_data.collect())\n",
    "local_optimizer = Adam(learning_rate=lr_schedule)\n",
    "local_model.compile(optimizer=local_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "local_model.fit(datagen.flow(train_data_array, train_labels, batch_size=batch_size),\n",
    "                steps_per_epoch=len(train_data_array) / batch_size,\n",
    "                epochs=epochs)\n",
    "\n",
    "end_local_time = time.time()\n",
    "print(f\"Time taken for local training: {end_local_time - start_local_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "local_test_loss, local_test_accuracy = local_model.evaluate(np.array(test_data.collect()), test_labels)\n",
    "\n",
    "print(f'Test accuracy for local model: {local_test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Save model to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"model-distributed-training.h5\")\n",
    "local_model.save(\"local-model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Stop Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
